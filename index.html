<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>IROS Workshop: Safety and Robustness in Reinforcement Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">About</a></li>
							<li><a href="#one">Program</a></li>
							<li><a href="#two">Invited Speakers</a></li>
							<li><a href="#four">Call for Papers</a></li>
							<li><a href="#three">Organization Committee</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<img src="images/RLCONFORM.png" style="float:right; margin:0px 0px 0px 0px; cursor:pointer; cursor:hand; border:0" width="250" height="220" alt="RLCONFORM logo">
							<h1>IROS'21 Workshop RL-CONFORM</h1>
							<h2>Reinforcement Learning meets HRI, Control, and Formal Methods </h2>
							<!-- <ul class="actions">
								<li><a href="#one" class="button scrolly">Learn more</a></li>
							</ul> -->
							<p>Reinforcement learning (RL) has shown remarkable achievements in applications ranging from autonomous driving, object manipulation, or beating best players in complex board-games. 
								However, elementary problems of RL remain open: exploratory and learned policies may cause unsafe situations, lack task-robustness, or be unstable. 
								By satisfactorily addressing these problems, RL research will have long-lasting impact and see breakthroughs on real physical systems and in human-centered environments. 
								<br />
								<br />
								Different communities have proposed multiple techniques to increase safety, transparency, and robustness of RL. 
								The aim of this workshop is to provide a multidisciplinary platform to (1) jointly identify and clearly define the major challenges in RL, (2) propose and debate existing approaches to ensure desired properties of learned policies from various perspectives, and (3) discuss benchmarks to accelerate RL research. 
								<!-- This could foster researchers’ awareness of progress and concerns from other communities, and promote collaborations among different communities, such as formal methods and machine learning to solve the challenges of RL.  -->
								The themes of the workshop would comprise (but not be limited to) RL and control theory, RL and Human-Robot Interaction, RL and Formal Methods, benchmarking of RL, etc. 
								<br />
							</p>
							<!--<p style="font-size: xx-large;"> <b>This page is under construction and contents may subject to change.</b> -->
						</p>
						</div>
					</section>

				
				<!-- One -->
					<section id="one" class="wrapper style3 fade-up">
						
						<div class="inner">
						<h2>Program</h2>

						<span style="font-weight: 400;">Tentative program (subject to changes – all times are CEST).</span>

						<p>&nbsp;</p>
						<table cellpadding=0 cellspacing=0>
						<tbody>
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">14:00-14:10</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Welcome</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">14:10-14:30</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Mohammed Chetouani</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">14:30-14:50</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Ufuk Topcu</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">14:50-15:20</span>
						</td>
						<td>
						<span style="font-weight: 400;">Short paper presentations I</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">15:20-15:30</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Break</span></em>
						</td>
						</tr>						
						
						<tr>
						<td>
						<span style="font-weight: 400;">15:30-15:50</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Emma Brunskill</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">15:50-16:10</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Dorsa Sadigh</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">16:10-16:30</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Kimin Lee</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">16:30-16:40</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Break</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">16:40-17:00</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Josh Achiam</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">17:00-17:20</span>
						</td>
						<td>
						<span style="font-weight: 400;">Invited talk: Fabio Ramos</span>
						</td>
						</tr>						
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">17:20-17:40</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Break</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">17:40-20:00</span>
						</td>
						<td>
						<span style="font-weight: 400;">Interactive Session</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">20:00-20:20</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Break</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<span style="font-weight: 400;">20:20-21:00</span>
						</td>
						<td>
						<span style="font-weight: 400;">Short paper presentations II</span>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 400;">21:00-</span></em>
						</td>
						<td>
						<em><span style="font-weight: 400;">Workshop outro and Get together</span></em>
						</td>
						</tr>
						</tbody>
						</table>
						<p>&nbsp;</p>
						
						
						<h3>Short paper presentations</h3>
												
						<p>&nbsp;</p>
						<table cellpadding=0 cellspacing=0>
						<tbody>						
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">14:50-15:00</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Jakob Thumm and Matthias Althoff.	"Formally Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">15:00-15:10</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Dennis Groß, Nils Jansen, Sebastian Junges and Guillermo Perez.	"COOL-MC: A Comprehensive Tool for Learning and Model Checking"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">15:10-15:20</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Filip Cano Cordoba.	"Improve RL by clever initialization with DPLL: a case study playing Super Mario Bros"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">20:20-20:30</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Bhoram Lee, Jonathan Brookshire and Supun Samarasekera. 	"Mixed-Reality Testbed for Robotic Systems with Human Interaction"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">20:30-20:40</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Michael McDonald and Dylan Hadfield-Menell.	"Guided Imitation of Task and Motion Planning"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">20:40-20:50</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Jerry Zhi-Yang He, Anca D. Dragan and Dylan Hadfield-Menell.	"Fast Inverse Reward Design with Universal Feature Networks"</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">20:50-21:00</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Tommaso Mannucci and Julio de Oliveira Filho.	"Runtime verification for reinforcement learning modules"</span></em>
						</td>
						</tr>
						
						</tbody>
						</table>
						<p>&nbsp;</p>
						
						
						
						
						
						
						<h3>Interactive Session</h3>
												
						<p>&nbsp;</p>
						<table cellpadding=0 cellspacing=0>
						<tbody>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">17:40-18:40</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Panel I</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">18:40-19:00</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Break</span></em>
						</td>
						</tr>
						
						<tr>
						<td>
						<em><span style="font-weight: 300;">19:00-20:00</span></em>
						</td>
						<td>
						<em><span style="font-weight: 300;">Panel II</span></em>
						</td>
						</tr>
						</tbody>
						</table>
						<p>&nbsp;</p>
						
						
						
						<!-- 
						<section>
							<a href="#" class="image"><img src="images/pic01.jpg" alt="" data-position="center center" /></a>
							<div class="content">
								<div class="inner">
									<h2>Sed ipsum dolor</h2>
									<p>Phasellus convallis elit id ullamcorper pulvinar. Duis aliquam turpis mauris, eu ultricies erat malesuada quis. Aliquam dapibus.</p>
									<ul class="actions">
										<li><a href="generic.html" class="button">Learn more</a></li>
									</ul>
								</div>
							</div>
						</section>
						<section>
							<a href="#" class="image"><img src="images/pic02.jpg" alt="" data-position="top center" /></a>
							<div class="content">
								<div class="inner">
									<h2>Feugiat consequat</h2>
									<p>Phasellus convallis elit id ullamcorper pulvinar. Duis aliquam turpis mauris, eu ultricies erat malesuada quis. Aliquam dapibus.</p>
									<ul class="actions">
										<li><a href="generic.html" class="button">Learn more</a></li>
									</ul>
								</div>
							</div>
						</section>
						<section>
							<a href="#" class="image"><img src="images/pic03.jpg" alt="" data-position="25% 25%" /></a>
							<div class="content">
								<div class="inner">
									<h2>Ultricies aliquam</h2>
									<p>Phasellus convallis elit id ullamcorper pulvinar. Duis aliquam turpis mauris, eu ultricies erat malesuada quis. Aliquam dapibus.</p>
									<ul class="actions">
										<li><a href="generic.html" class="button">Learn more</a></li>
									</ul>
								</div>
							</div>
						</section>
						-->
						</div>
					</section>

				<!-- Two -->
					<section id="two" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Invited Speakers</h2>
							
							<section>
							<div class="content">
								<div class="inner">
									<h3><a href="https://www.linkedin.com/in/joshua-achiam-13887199"> Josh Achiam </a>, OpenAI, USA.</h3>
									<img src="images/josh_achiam_lr.png" width="300" style="float:right"; alt="" />
									<p> Title: Safe Exploration with Constrained Reinforcement Learning</p>
									<p> Abstract: this talk, we will advocate for constrained reinforcement learning as a key formalism for safety in the context of safe exploration problems and safe RL more broadly. We will discuss challenges in benchmarking progress in safe exploration, and present Safety
 Gym, a suite of environments and tools intended to meet that need. We will additionally present Lagrangian PID methods, a recent algorithmic development that improves the state of the art for constraint satisfaction and robustness as measured with Safety Gym
 environments. Finally, we will conclude with a discussion of the gaps between safety research and safety practice, and describe how we might use lessons from the engineering discipline of systems safety to inform our future research. </p>
									<p> Bio: Joshua Achiam is a research scientist at OpenAI working on safety for general-purpose AI. He received his PhD from UC Berkeley in 2021, where he was coadvised by Pieter Abbeel and Shankar Sastry. His research covers a range of topics in safety and exploration
 in deep reinforcement learning, and his interests include alignment, optimization, ethics, and systems safety.  </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
							
							<section>
							<div class="content">
								<div class="inner">
									<h3> <a href="https://cs.stanford.edu/people/ebrun/"> Emma Brunskill </a>, Stanford University, USA.</h3>
									<!--img src="images/emmabrunskill.png" width="300" style="float:right"; alt="" /-->
									<p> Title: Learning More from the Past: Offline Batch RL </p>
									<p> Abstract: There is a huge opportunity for enhancing evidence-driven decision making by leveraging the increasing amount of data collected about decisions made, and their outcomes. Reinforcement learning is a natural framework to capture this setting, but online reinforcement learning may always be feasible for higher stakes domains like healthcare or education. In this talk I will discuss our work on offline, batch reinforcement learning, and the progress we have made in techniques that can work efficiently with limited data, and under limited assumptions about the domain. </p>
									<p> Bio: Emma Brunskill is an associate professor in the Computer Science Department at Stanford University. Her lab is part of the Stanford AI Lab, the Stanford Statistical ML group, and AI Safety @Stanford. Brunskill and her group's work has been honored by early faculty career awards (National Science Foundation, Office of Naval Research, Microsoft Research (1 of 7 worldwide) ) and several best research paper nominations (CHI, EDMx3) and awards (UAI, RLDM, ITS). </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
							
							
							<section>
							<div class="content">
								<div class="inner">
									<h3> <a href="https://www.isir.upmc.fr/personnel/chetouani/"> Mohamed Chetouani </a>, Sorbonne University, France.</h3>
									<img src="images/MohamedChetouani.png" width="300" style="float:right"; alt="" />
									<p> Abstract: Socially Interactive Learning: On the interpretation of human teaching signals </p>
									<p> There are increasing situations in which humans and AI systems are acting, deciding and/or learning. In this short talk, we discuss approaches and models able to capture specific strategies of humans while they are teaching agents. We will see how social learning based approaches make it possible to take into account such strategies in the development of interactive machine learning techniques and in particular when it comes to social robotics. </p>
									<p> Bio: Prof. Mohamed Chetouani is the head of the PIRoS (Perception, Interaction et Robotique Sociales) research team at the Institute for Intelligent Systems and Robotics (CNRS UMR 7222), Sorbonne University. He is currently a Full Professor in signal processing and machine learning for human-machine interaction. His activities cover social signal processing, social robotics and interactive machine learning with applications in psychiatry, psychology, social neuroscience and education. He is the Deputy Director of the Laboratory of Excellence SMART Human/Machine/Human Interactions In The Digital Society.  Since 2018, he is the coordinator of the ANIMATAS H2020 Marie Sklodowska Curie European Training Network. He is the local co-chair of IEEE ICRA 2020 (Paris), Program co-chair of ICMI 2020 (Utrecht), General Chair of VIHAR 2021 and of ACM ICMI 2023. Since 2020, he is the President of the Ethical Advisory Board of Sorbonne University. </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
													
							
							<section>
							<div class="content">
								<div class="inner">
									<h3> <a href="https://sites.google.com/view/kiminlee"> Kimin Lee </a>, UC Berkeley, USA.</h3>
									<img src="images/kimin-lee-thumb-2272.png" width="300" style="float:right"; alt="">
									<p> Title: Scaling Up Deep Reinforcement Learning via Preference-based Learning: Feedback-efficient Algorithm and Benchmark </p>
									<p> Abstract: Deep Reinforcement Learning (RL) has been successful in a range of challenging domains, such as board games, video games, and robotic control tasks. Scaling RL to many applications, however, is yet precluded by a number of challenges. One such challenge lies in designing a suitable reward function that is sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this talk, I’ll present PEBBLE: a feedback-efficient RL algorithm by which learning is largely autonomous and supplemented by a practical number of preferences provided by a supervisor. Based on off-policy RL and unsupervised pre-training, our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions. Additionally, I’ll introduce B-Pref: a benchmark specially designed for preference-based RL to further our understanding of the strengths of existing algorithms. . </p>
									<p> Bio: Kimin Lee is a postdoc at UC Berkeley working with Prof. Pieter Abbeel. He is interested in the directions that enable scaling deep reinforcement learning to diverse and challenging domains — human-in-the-loop reinforcement learning, unsupervised reinforcement learning, and reinforcement learning from pixels. He received his Ph.D. from KAIST, where he worked on a reliable and robust machine/deep learning with Prof. Jinwoo Shin. During Ph.D., he also interned and collaborated closely with Prof. Honglak Lee at University of Michigan. </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
							
							
							<section>
							<div class="content">
								<div class="inner">
									<h3>Fabio Ramos, University of Sydney, Australia, and NVIDIA, USA.</h3>
									<!--img src="images/fabioramos.png" width="300" style="float:right"; alt="" /-->
									<p> Title: TBD </p>
									<p> Abstract: TBD. </p>
									<p> Bio: TBD. </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
							
							
							<section>
							<div class="content">
								<div class="inner">
									<h3> <a href="https://dorsa.fyi">Dorsa Sadigh</a>, Stanford University, USA.</h3>
									<img src="images/dorsa.png" width="300" style="float:right"; alt="" >
									<p> Title: Bringing in the Human in the (Reinforcement) Learning Loop </p>
									<p> Abstract: Today, I will talk about the challenges of bringing in humans in the learning loop. Specifically, I will discuss how we can be more data-efficient when learning from humans by actively querying them or tapping into many different sources of data. I will then connect these offline learning ideas to new paradigms of learning online from interactions with humans within the RL framework to improve human-robot and multi-agent interactions. </p>
									<p> Bio: Dorsa Sadigh is an assistant professor in Computer Science and Electrical Engineering at Stanford University.  Her research interests lie in the intersection of robotics, learning, and control theory. Specifically, she is interested in developing algorithms for safe and adaptive human-robot and multi-agent interaction. Dorsa received her doctoral degree in Electrical Engineering and Computer Sciences (EECS) from UC Berkeley in 2017, and received her bachelor’s degree in EECS from UC Berkeley in 2012.  She is recognized by awards such as the NSF CAREER award, the AFOSR Young Investigator award, the IEEE TCCPS early career award, MIT TR35, as well as industry awards such as the JP Morgan, Google, and Amazon faculty research awards. </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
								</div>
							</div>
							</section>
							
							
							<section>
							<a href="#" class="image"></a>
							<div class="content">
								<div class="inner">
									<h3> <a href="https://www.oden.utexas.edu/people/1430/"> Ufuk Topcu </a>, University of Texas at Austin, USA. </h3>
									<img src="images/Topcu-1-cut-small.png" width="300" style="float:right"; alt="" />
									<p> Title: Verifiable reinforcement learning systems </p>
									<p> Abstract: Reinforcement learning algorithms offer tremendous capabilities in systems that work in unknown environments. However, there remain significant barriers to their deployment in safety-critical engineering applications. For example, the verification of reinforcement learning systems is complex, and this is particularly true of monolithic end-to-end learning approaches that have been gaining popularity. In this presentation, I focus on the following question: How can we build trustworthy engineering systems that rely on reinforcement learning? I will present a series of recent results that seek answers for this question at the intersection of formal methods and reinforcement learning. I will detail two of these results on (i) verifiable and compositional reinforcement learning and (ii) cooperative multi-agent reinforcement learning. </p>
									<p> Bio: Ufuk Topcu is an Associate Professor in the Department of Aerospace Engineering and Engineering Mechanics at The University of Texas at Austin. He is a core faculty member at the Oden Institute for Computational Engineering and Sciences. Ufuk obtained his Doctor of Philosophy degree from the University of California, Berkeley in 2008. Prior to joining The University of Texas at Austin, he was with the Department of Electrical and Systems Engineering at the University of Pennsylvania. He was a postdoctoral scholar at California Institute of Technology until 2012. Ufuk’s research focuses on the theoretical and algorithmic aspects of the design and verification of autonomous systems, typically in the intersection of formal methods, reinforcement learning, and control theory. He takes a relatively broad view on autonomy and tends to tackle abstract problems motivated by challenges cutting across multiple applications of autonomy. His research contributions have been recognized by the NSF CAREER Award, the Air Force Young Investigator Award, the IEEE CSS Antonio Ruberti Young Researcher Prize, and Oden Institute Distinguished Researcher Award. </p>
									<!-- <ul class="actions"> -->
										<!-- <li><a href="generic.html" class="button">Learn more</a></li> -->
									<!-- </ul> -->
									
								</div>
							</div>
							</section>
							
							
						</div>
					</section>

				<!-- Three -->
					
									<!-- Four -->
					<section id="four" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>Call for Papers</h2>
							
							
							<p> We invite extended abstract submissions of recent works, preliminary work with open questions is very welcome, related to the theme of the workshop. All accepted abstracts will be part of a short paper presentation session held during the workshop, where the authors will have the opportunity to present their lines of work in a 2-minutes-long video that will be played during the workshop, and followed by a 3-minutes live Q&A session. This is a non-archival venue: there will be no formal proceedings, but we encourage the authors to publish their extended abstracts on Arxiv (where the link will be placed on the workshop’s website). Abstracts may be submitted to other venues in the future. </p>
							
							<p> Topics of interest include (but are not limited to) applications of Reinforcement Learning to:
								<ul>
								  <li>Formal Methods</li>
								  <li>Control</li>
								  <li>Human-Robot Interaction</li>
								</ul>
							</p>


							<h3>Important details</h3>

							<p> 
								<ul>
								  <li>When: September 27, 2021.</li>
								  <li>Website: https://saferl-workshop.github.io/RL-CONFORM_IROS2021/</li>
								  <li>Submission deadline: <s> August 13, 2021 </s> September 1, 2021 (AoE)</li>
								  <li>Notification of acceptance: <s> August 31, 2021 </s> September 7, 2021 (AoE)</li>
								  <li>Submission website: <a href="https://easychair.org/conferences/?conf=rlconform2021"> https://easychair.org/conferences/?conf=rlconform2021 </a> </li>
								  <li>Submission format: 2-page papers/extended abstracts (plus references) of original, possibly ongoing research. Papers should be formatted in the IROS 2021 style guidelines  <a href="https://www.iros2021.org/call-for-papers"> https://www.iros2021.org/call-for-papers </a> </li>
								</ul>
							</p>
							

							<!-- <ul class="actions">
								<li><a href="generic.html" class="button">Learn more</a></li>
							</ul> -->
						</div>
					</section>
					
					
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Organizing Committee and Support</h2>
							
							<h3>Program Chairs </h3>
							<p><ul>
								  <li><a href="https://www.kth.se/profile/pek2"> Christian Pek, KTH Royal Institute of Technology, Sweden. </a> </li>
								  <li><a href="https://engineering.mit.edu/faculty/dylan-hadfield-menell/"> Dylan Hadfield-Menell, MIT, USA. </a> </li>
								  <li><a href="https://www.iaik.tugraz.at/person/bettina-koenighofer/"> Bettina Könighofer, TU Graz, Austria. </a> </li>
								  <li><a href="https://www.kth.se/profile/linard"> Alexis Linard, KTH Royal Institute of Technology, Sweden. </a> </li>
								  <li><a href="https://www.kth.se/profile/hyin"> Hang Yin, KTH Royal Institute of Technology, Sweden. </a> </li>
								</ul></p>
							<row>
							    <img src="images/profile_chris.jpg" width="150">
							    <img src="images/profile_dylan.jpg" width="150">
							    <img src="images/profile_bettina.jpg" width="150">
							    <img src="images/profile_alexis.jpg" width="150">
							    <img src="images/profile_hang.jpg" width="150">
							</row>
								
							<h3>Advisory Committee </h3>
							<p><ul>
								  <li><a href="https://people.eecs.berkeley.edu/~anca/"> Anca Dragan, UC Berkeley, USA. </a> </li>
								  <li><a href="http://www.cs.ru.nl/personal/nilsjansen/"> Nils Jansen, Radboud University Nijmegen, Netherlands. </a> </li>
								  <li><a href="https://www.kth.se/profile/dani"> Danica Kragic Jensfelt, KTH Royal Institute of Technology, Sweden. </a> </li>
								  <li><a href="https://www.kth.se/profile/iolanda"> Iolanda Leite, KTH Royal Institute of Technology, Sweden. </a> </li>
								  <li><a href="https://www.kth.se/profile/tumova"> Jana Tumova, KTH Royal Institute of Technology, Sweden. </a> </li>
								</ul></p>

							<h3>This workshop is supported by </h3>
							<p><ul>
								  <li><a href="https://www.ieee-ras.org/algorithms-for-planning-and-control-of-robot-motion"> IEEE RAS Technical Committee on Algorithms for Planning and Control of Robot Motion </a>  </li>
								  <li><a href="https://www.ieee-ras.org/robot-learning"> IEEE RAS Technical Committee on Robot Learning </a> </li>
								</ul></p>
							
						</div>
					</section>
					
					
			</div>
			
			
			
				
		<!-- Footer -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>


	</body>
</html>
